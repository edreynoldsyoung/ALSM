---
title: "Applied Linear Statistical Models"
author: "Ed Young"
date: "9/27/2019"
output:
     html_document:
          theme: null
          highlight: null
          css: styles.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE, warning=FALSE, message=FALSE)
```

<!-- Load Libraries and Data -->
```{r, include=FALSE}
library(tidyverse)
library(broom)
library(latex2exp)
library(data.table)
library(ggfortify)
library(randtests)
library(MASS)
library(lmtest)

df20<-fread("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%201%20Data%20Sets/CH01PR20.txt", col.names=c("Y","X"))
df21<-fread("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%201%20Data%20Sets/CH01PR21.txt", col.names=c("Y","X"))
df27<-fread("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%201%20Data%20Sets/CH01PR27.txt", col.names=c("Y","X"))

```
# Chapter 3
## Departures from the model to be studied by residuals 
### The regression function is not linear (functional misspecification)  
#### Diagnostic Plots of Residuals
##### Scatter plot of the Independent Variable against the Dependent Variable with a lowess (loess) Fit Line.
##### Plot residuals against the predictor variable
##### Plot residuals against the fitted values
#### Statistical Tests
##### Lack of Fit Test (F Test)
#### Remedial Measures
##### Consider non-linear models: 
###### Polynomial Regression
###### Piecewise Regression
### The error terms do not have constant variance (heteroskedacity)  
#### Diagnostic Plots of Residuals
##### plot residuals against the predictor variable
##### plot residuals against the fitted values
##### Plot of absolute or squared residuals against predictor variable and fitted values
#### Statistical Tests
#### Remedial Measures
### The error terms are not independent (serial corelation of error terms)  
#### Diagnostic Plots of Residuals
##### Plot of residuals against time or other sequence
#### Statistical Tests
#### Remedial Measures
### The model fits all but one or a few outlier observations  
#### Diagnostic Plots of Residuals

plot residuals against the predictor variable

plot residuals against the fitted values - especially using semistudentized residuals

Box plot, stem-and-leaf plots, and dot plots of residuals

#### Statistical Tests

#### Remedial Measures

### The error terms are not normally distributed (structural changes in error terms)

#### Diagnostic Plots of Residuals

Box plot, stem-and-leaf plots, and dot plots of residuals

Normal probability plot of residuals

#### Statistical Tests

#### Remedial Measures

### One or several important predictor variables have been omitted from the model.

#### Diagnostic Plots of Residuals

Plot residuals against variables omitted from the model (such as time)

#### Statistical Tests

#### Remedial Measures

## Dignostics of Residuals

### Scatter plot of the Independent Variable against the Dependent Variable with a lowess (loess) Fit Line.
### Plot of residuals against predictor variable
### Plot of absolute or squared residuals against predictor variable and fitted values
### Plot of residuals against fitted values
### Plot of residuals against time or other sequence
### Plots of residuals against omitted predictor variables
### Box plot, stem-and-leaf plots, and dot plots of residuals
### Normal probability plot of residuals

## Statistical Tests

### Runs Test
### Correlation between the ordered residuals and their expected values
### Durbin-Watson Test
### Brown-Forsythe Test
### Breusch-Pagan Test
### Hat Matrix (see Ch 10)
### Chi-Square test
### Kolmogorov-Smirnov Test
### Lilliefors Test
### Lack of Fit Test (F Test)

## Remedial Measures

### Abandon Linear Regression Model and develop more appropriate model
#### Non Linear Model
#### Multiple Regression Model
#### Robust Regression Models
#### Weighted Least Squares
#### Correlated Error Models
### Transformations
#### Simple Transformations
##### $X^`=\sqrt{X}$
##### $X^`=X^2$
##### $X^`=\log_{10}$
##### $X^`=\frac{1}{X}$
#### Box-Cox Transformations
#### Shape of Regression Function
##### moving averages
##### running medians
##### Lowess

## Useful Formulas

****

### Semi-studentized Residuals

<span style="display:block" id="math">$e_i^* = \frac{e_i}{\sqrt{MSE}}$</span>

****

### Normal Probability Plot of residuals

<span style="display:block" id="note"> k is the value of the ranked residual</span>
<span style="display:block" id="note"> z(A) is the inverse look-up of normal distribution (qnorm)</span>

<span style="display:block" id="math">$\hat{k}_i = \sqrt{MSE} \left [ z(\frac{k_i - .375}{n+.25}) \right ]\\$</span>

****

### Brown-Forsythe Test for constancy of error variance

<span style="display:block" id="note"> Devide the sample into two groups:</span>
<span style="display:block" id="note"> Group 1: residuals $e_{i1}$; sample size=$n_1$; group median $d_{i1}=|e_{i1}-\tilde{e}_i|$ </span>


<span style="display:block" id="math">
$n=n_1+n_2\\
d_{i1}=|e_{i1}-\tilde{e}_1|\\
d_{i2}=|e_{i2}-\tilde{e}_2|\\
s^2=\frac{\sum(d_{i1}-\bar{d}_1)^2+\sum(d_{i2}-\bar{d}_2)^2}{n-2}\\
t^*_{BF}=\frac{\bar{d_1}-\bar{d_2}}{s\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\\
\textrm{Decision Rule:}\\
\quad H_0: \textrm{error variance is constant}\\
\quad H_a: \textrm{error variance is not constant}\\
\textrm{If }|t^*_{BF}|\leq t(1-\frac{\alpha}{2}; n-2)\textrm{ then conclude }H_0\\
\textrm{If }|t^*_{BF}|> t(1-\frac{\alpha}{2}; n-2)\textrm{ then conclude }H_a$
</span>

****

### Breusch-Pagan Test for constancy of error variance

<span style="display:block" id="math">
$log_e \sigma_i^2 = \gamma_0 + \gamma_1 X_i\\
\textrm{SSR is obtained from regressing the squared residuals } e_i^2 \textrm{ against } X_i\\
\textrm{SSE is obtained from regressing Y against X }\\
X_{BP}^2=\frac{\frac{SSR^*}{2}}{\left ( \frac{SSE}{n} \right )^2}\\
\textrm{Decision Rule:}$
<span>

<span style="display:block" id="note"> Constancy of error variance corresponds to $\gamma_1=0$</span>

<span style="display:block" id="math">
$\quad H_0: \gamma_1=0\\
\quad H_a: \gamma_1 \neq 0\\
\textrm{If }|X^2_{BP}|\leq \chi^2(1-\alpha; 1)\textrm{ then conclude }H_0\\
\textrm{If }|X^2_{BP}|>  \chi^2(1-\alpha; 1)\textrm{ then conclude }H_a$
</span>

        m<-lm(Y~X, data=DATA)
        t<-augment(m)
        n<-length(DATA$X)
        alpha<-.05
        m1<-lm((t$.resid)^2~t$X)
        t1<-tidy(aov(m1))
        SSR<-t1$sumsq[1]
        t2<-tidy(aov(m))
        SSE<-t2$sumsq[2]
        X2BP<-(SSR/2)/((SSE/n)^2)
        CHI2<-qchisq(1-alpha,1)


****

### Lack-of-Fit Test

<span style="display:block" id="math">
$\sum_{i}\sum_{1\le j \le n_i}(Y_{ij}-\hat{Y}_i)^2=\sum_{i}\sum_{1 \le j \le n_i}(Y_{ij}-\bar{Y}_i)^2 + \sum_{i}n_i(\bar{Y}_i - \hat{Y}_i)^2$
</span>

## (Problem 3.4) 

Refer to **Copier maintenance** Problem 1.20

```{r}
m20<-lm(Y~X, data=df20)
t20<-augment(m20)
```

###  Prepare a dot plot for the number of copiers serviced $X_i$.  What information is provided by this plot?  Are there an outlying cases with respect to this variable?

```{r}
ggplot(data=df20, aes(x=X))+
     geom_dotplot()
```

The values are fairly evenly spread between 0 and 10, with no outliers.

###  The cases are given in time order.  Prepare a time plot for the number of copiers serviced.  What does your plot show?

```{r}
plot(df20$Y, type="l", main="Index Plot", ylab="Copiers Serviced", pch=19)
abline(h=0)
```

There does not seem to be any relationship with time.

###  Prepare a stem-and-leaf plot of the residuals.  Are there any noteworthy features in this plot?

```{r}
m20$residuals
stem(m20$residuals, scale=2)
```

The residuals seem normally distributed.

###  Prepare residual plots of $e_i$ versus $\hat{Y}$ and $e_i$ versus $X_i$ on separate graphs.  Do these plots provide the same information?  What departures from regression model (2.1) can be studied from these plots?  State your findings.

```{r}
ggplot(data=t20)+
     geom_point(mapping=aes(x=.fitted, y=.resid)) + 
     geom_hline(yintercept=0) +
     labs(title=TeX('$\\hat{Y}$   vs.  $e_i$'), 
          x=TeX('$\\hat{Y}$'), 
          y=TeX('$e_i$'))
ggplot(data=t20) + 
     geom_point(mapping=aes(x=X, y=.resid)) +
     geom_hline(yintercept=0) +
     labs(title=TeX('X vs. $e_i$'), 
          x="X", 
          y=TeX('$e_i$'))
```

It appears that the mean of the residuals is 0 in regard to both $X$ and $\hat{Y}$ and that the variance in the residuals is constant with regard to $X$ or $\hat{Y}$.  Tis is consistent with the regression model (2.1).

###  Prepare a normal probability plot of the residuals.  Also obtain the coefficient of correlation between the ordered residuals and their expected values under normality.  does the normality assumption appear to be tenable here?  Use $\alpha=.05$

```{r}
qqnorm(rstandard(m20),
       ylab="Standardized Residuals",
       xlab="Normal Scores",
       main="Normal Probability Plot of the residuals"
       )
qqline(rstandard(m20))
t<-tidy(aov(m20))
SSR<-t$sumsq[1]
SSE<-t$sumsq[2]
SSTO<-SSR+SSE
R2<-SSR/SSTO
r<-sqrt(R2)
```

The coefficient of correlation between the ordered residuals and their expected values under normality is `r r`

###  Prepare a time plot of the residuals to ascertain whether the error terms are correlated over time.  What is your conclusion?

```{r}
plot(m20$residuals, type="l", main="Index Plot", ylab="Residuals", pch=19)
abline(h=0)
```

The residuals appear to be randomly distruted over time.

###  Assume that $log_e \sigma_i^2 = \gamma_0 + \gamma_1 X_i$  Conduct the Breusch-Pagan test to determine whether or not the error variance varies with the level of $X$.  Use $\alpha = .05$  State the alternatives, decision rule, and conclusion.

The Breusch-Pagan Test uses the following formulas:

$\textrm{SSR is obtained from regressing the squared residuals } e_i^2 \textrm{ against } X_i\\
\textrm{SSE is obtained from regressing Y against X }\\
X_{BP}^2=\frac{\frac{SSR^*}{2}}{\left ( \frac{SSE}{n} \right )^2}\\$

$\textrm{Decision Rule:}\\
\quad H_0: \gamma_1=0\\
\quad H_a: \gamma_1 \neq 0\\
\textrm{If }|X^2_{BP}|\leq \chi^2(1-\alpha; 1)\textrm{ then conclude }H_0\\
\textrm{If }|X^2_{BP}|>  \chi^2(1-\alpha; 1)\textrm{ then conclude }H_a$

```{r}
n<-length(df20$X)
alpha<-.05
m1<-lm((t20$.resid)^2~t20$X)
t1<-tidy(aov(m1))
SSR<-t1$sumsq[1]
t2<-tidy(aov(m20))
SSE<-t2$sumsq[2]
X2BP<-(SSR/2)/((SSE/n)^2)
CHI2<-qchisq(1-alpha,1)
```

$|X_{BP}^2|=$ `r X2BP`

$\chi^2(.95; 1)=$ `r CHI2`

Because $|X_{BP}^2|<\chi^2(1-\alpha; 1)$ I conclude $H_0$ that the error variance is constant with levels of X.

```{r}
bp20<-bptest(m20, studentize=FALSE, data=df20)
bp20
```

The p-value is larger than $\alpha$ ( `r bp20$p.value[1]` > `r alpha` ), therefore I would not reject $H_0$.

## (Problem 3.5) 

Refer to **Airfreight breakage** Problem 1.21

```{r}
m21<-lm(Y~X, data=df21)
t21<-augment(m21)
```

###  Prepare a dot plot for the number of transfers $X_i$.  Does the distribution of the number of tranfers appear to be asymmetrical?

```{r}
ggplot(data=df21, aes(x=X))+
     geom_dotplot()
```

The distribution does appear asymmetrical, with more breaks happening at low transfers than at high.

###  The cases are given in time order.  Prepare a time plot for the number of tranfers.  Is any systematic pattern evident in your plot?  Discuss.

```{r}
plot(df21$X, type="l", main="Index Plot", ylab="Number of Transfers", pch=19)
abline(h=0)
```

There does not seem to be any systematic pattern evident in the time plot.

###  Obtain the residuals $e_i$ and prepare a stem-and -leaf plot of the residuals.  What information is provided by your plot?

```{r}
m21$residuals
stem(m21$residuals, scale=2)
```

The residuals are fairly evenly distributed, but appear bimodal.

###  Plot the residuals $e_i$ against $X_i$ to ascertain whether any departures from regression model (2.1) are evident.  What is your conclusion?

```{r}
ggplot(data=df21, aes(X,Y))+
     geom_point()+
     geom_smooth()
ggplot(data=t21)+
     geom_point(mapping=aes(x=.fitted, y=.resid)) + 
     geom_hline(yintercept=0) +
     labs(title=TeX('$\\hat{Y}$   vs.  $e_i$'), 
          x=TeX('$\\hat{Y}$'), 
          y=TeX('$e_i$'))
ggplot(data=t21) + 
     geom_point(mapping=aes(x=X, y=.resid)) +
     geom_hline(yintercept=0) +
     labs(title=TeX('X vs. $e_i$'), 
          x="X", 
          y=TeX('$e_i$'))
```

By plotting the dependant variable against the independant variable it appears that a linear model may fit reasonably well.  However, when the residuals are plotted against the independant variable, and against the fitted dependant variable, it appears that the variance of the residuals is not constant.

###  Prepare a normal probability plot of the residuals.  Also obtain the coefficeint of corretaion between the ordered residuals and their expected values under normality to ascertain whether the normality assumption is reasonable here.  Use $\alpha=.01$.  What do you conclude?

```{r}
qqnorm(rstandard(m21),
       ylab="Standardized Residuals",
       xlab="Normal Scores",
       main="Normal Probability Plot of the residuals"
       )
qqline(rstandard(m21))
t<-tidy(aov(m21))
SSR<-t$sumsq[1]
SSE<-t$sumsq[2]
SSTO<-SSR+SSE
R2<-SSR/SSTO
r<-sqrt(R2)
```

The coefficient of correlation between the ordered residuals and their expected values under normality is `r r`

###  Prepare a time plot of the residuals.  What information is provided by your plot?

```{r}
plot(m21$residuals, type="l", main="Index Plot", ylab="Residuals", pch=19)
abline(h=0)
```


###  Conduct the Breusch-Pagan test to determine whether or not the error variance varies with the level of $X$.  Use $\alpha=.10$.  State the alternatives, decision rule, and conclusion.  Does your conclusion support your prelimiary findings in part (d)?

The Breusch-Pagan Test uses the following formulas:

$\textrm{SSR is obtained from regressing the squared residuals } e_i^2 \textrm{ against } X_i\\
\textrm{SSE is obtained from regressing Y against X }\\
X_{BP}^2=\frac{\frac{SSR^*}{2}}{\left ( \frac{SSE}{n} \right )^2}\\$

$\textrm{Decision Rule:}\\
\quad H_0: \gamma_1=0\\
\quad H_a: \gamma_1 \neq 0\\
\textrm{If }|X^2_{BP}|\leq \chi^2(1-\alpha; 1)\textrm{ then conclude }H_0\\
\textrm{If }|X^2_{BP}|>  \chi^2(1-\alpha; 1)\textrm{ then conclude }H_a$

```{r}
n<-length(df21$X)
alpha<-.10
m1<-lm((t21$.resid)^2~t21$X)
t1<-tidy(aov(m1))
SSR<-t1$sumsq[1]
t2<-tidy(aov(m21))
SSE<-t2$sumsq[2]
X2BP<-(SSR/2)/((SSE/n)^2)
CHI2<-qchisq(1-alpha,1)
```

$|X_{BP}^2|=$ `r X2BP`

$\chi^2(.90; 1)=$ `r CHI2`

Because $|X_{BP}^2|\leq\chi^2(1-\alpha; 1)$ I conclude $H_0$ that the error variance is constant with levels of X.

```{r}
bp21<-bptest(m21, studentize=FALSE, data=df21)
bp21
```

The p-value is larger than $\alpha$ ( `r bp21$p.value[1]` > `r alpha` ), therefore I would not reject $H_0$.

## (Problem 3.7) 

Refer to **Muscle mass** Problem 1.27

```{r}
m27<-lm(Y~X, data=df27)
t27<-augment(m27)
```

###  Prepare a stem-and-leaf plot for the ages $X_i$.  Is this plot consistent with the random selection of women from each 10-year age group?  Explain.

```{r}
stem(df27$X, scale=2)
```

###  Obtain the residuals $e_i$ and prepare a dot plot of the residuals.  What does your plot show?

```{r}
stem(m27$residuals, scale=2)
```

###  Plot the residuals $e_i$ against $\hat{y}_i$ and also against $X_i$ on separate graphs to ascertain whether any departures from regression model 2.1 are evident.  Do th two plots provide the same information?  State your conclusion.


```{r}
ggplot(data=df27, aes(X,Y))+
     geom_point()+
     geom_smooth()
ggplot(data=t27)+
     geom_point(mapping=aes(x=.fitted, y=.resid)) + 
     geom_hline(yintercept=0) +
     labs(title=TeX('$\\hat{Y}$   vs.  $e_i$'), 
          x=TeX('$\\hat{Y}$'), 
          y=TeX('$e_i$'))
ggplot(data=t27) + 
     geom_point(mapping=aes(x=X, y=.resid)) +
     geom_hline(yintercept=0) +
     labs(title=TeX('X vs. $e_i$'), 
          x="X", 
          y=TeX('$e_i$'))
```


###  Prepare a normal probability plot of the residuals.  Also obtain the coefficient of correlation between the ordered residuals and their expected values under normality to ascertain whether the normality assumption is tenable here.  Use $\alpha=.10$

```{r}
qqnorm(rstandard(m27),
       ylab="Standardized Residuals",
       xlab="Normal Scores",
       main="Normal Probability Plot of the residuals"
       )
qqline(rstandard(m27))
t<-tidy(aov(m27))
SSR<-t$sumsq[1]
SSE<-t$sumsq[2]
SSTO<-SSR+SSE
R2<-SSR/SSTO
r<-sqrt(R2)
```

The coefficient of correlation between the ordered residuals and their expected values under normality is `r r`

###  Conduct the Breusch-Pagan test to determine whether or not the error variance varies with the level of $X$.  Use $\alpha=.01$. State the alternatives, decision rule, and conclusion.  Is your conclusion consistent with your preliminary findings in part (c)?


## (Problem 3.13) 

Refer to **Copier maintenance** Problem 1.20

###  What are the alternative conclusion when testing for lack of fit of a linear regression funtion?

###  Perform the test indicated in part (a).  Control the risk of Type I error at .05.  State the decion rule and conlcusion.

###  Does the test in part (b) detect other departures from regression model 2.1, such as lack of constant variance or lack or normality in the error terms?  Could the results of the test of lack of fit be affected by such departures?  Discuss.


## (Problem 3.17) 

**Sales growth**  A marketing researcher studied annual sales of a product that had beenintroduced 10 years ago.  The data are as follow, where $X$ is the year (coded) and $Y$ is sales in thousands of units.

###  Prepare a scatter plot of the data.  Does a linear relation appear adequate here?

###  Use the Box-Cox proceudre and standardization (3.36) to find an appropriate power transformation of $Y$.  Evaluate SSE for $\lambda =.3,.4,.5,.6,.7$.  What transformation of $Y$ is suggested?  

###  Use the transformation $Y^t=\sqrt{Y}$ and obtain the estimated linear regression function for the transformed data.

###  Plot the estimated regression line and the transformed data.  Does theregression line appear to be a good fit to the transformed data?

###  Obtain the residuals and plot the against the fitted values.  Also prepare a normal probability plot.  What do your plots show?

###  Express the estimated ression function in the original units.


